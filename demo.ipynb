{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this demonstration notebook for the Weather Classification Dataset and our trained models. This notebook provides an overview of the dataset, explores its structure, and showcases the performance of various models trained for weather classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from data_loading import download_dataset, WeatherDataModule, get_val_transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming download_dataset() is provided\n",
    "dataset_path = \"data/weather-dataset\"\n",
    "# download_dataset(dataset_path)\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# List dataset contents\n",
    "classes = sorted(os.listdir(dataset_path))  # Sort for consistent ordering\n",
    "num_classes = len(classes)\n",
    "print(f\"Number of Classes: {num_classes}\")\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "# Count total images\n",
    "num_images = sum(len(os.listdir(os.path.join(dataset_path, cls))) for cls in classes)\n",
    "print(f\"Total Number of Images: {num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset_path, classes, images_per_row=3):\n",
    "    num_classes = len(classes)\n",
    "    rows = (num_classes + images_per_row - 1) // images_per_row\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, images_per_row, figsize=(images_per_row * 3, rows * 3))\n",
    "    axes = np.array(axes).reshape(-1)  # Flatten axes array for easier indexing\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, cls)\n",
    "        sample_image = np.random.choice(os.listdir(class_path), size=1)[0]\n",
    "        \n",
    "        img_path = os.path.join(class_path, sample_image)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(cls, fontsize=12)\n",
    "    \n",
    "    # Hide unused subplots if number of classes is not a multiple of images_per_row\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(dataset_path, classes, images_per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count images per class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(dataset_path, cls))) for cls in classes}\n",
    "\n",
    "# Sort by class name (optional)\n",
    "class_counts = dict(sorted(class_counts.items()))\n",
    "\n",
    "# Print note about class imbalance\n",
    "print(\"As we can see in the following plot, the dataset has an uneven class distribution.\")\n",
    "print(\"During training, this is handled using class weights to balance the impact of each class.\")\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color=\"royalblue\")\n",
    "plt.xlabel(\"Weather Classes\", fontsize=12)\n",
    "plt.ylabel(\"Number of Images\", fontsize=12)\n",
    "plt.title(\"Class Distribution in Weather Classification Dataset\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "datamodule = WeatherDataModule(\"./data/weather-dataset\", 32, 1, get_val_transforms(), get_val_transforms())\n",
    "datamodule.setup()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "X_test = []\n",
    "y_test = []\n",
    "for images, labels in test_dataloader:\n",
    "    X_test.append(images)\n",
    "    y_test.append(labels)\n",
    "X_test = torch.cat(X_test)\n",
    "y_test = torch.cat(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "model_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing predictions from different models\n",
    "# Example: model_predictions = {\"Model_1\": y_pred_1, \"Model_2\": y_pred_2, ...}\n",
    "model_scores = {}\n",
    "y_true = y_test\n",
    "\n",
    "for model_name, y_pred in model_predictions.items():\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    model_scores[model_name] = {\"Accuracy\": accuracy, \"F1-Score\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names, accuracy, and F1-score\n",
    "model_names = list(model_scores.keys())\n",
    "accuracies = [model_scores[m][\"Accuracy\"] for m in model_names]\n",
    "f1_scores = [model_scores[m][\"F1-Score\"] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))  # X-axis positions\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "bar_width = 0.4\n",
    "plt.bar(x - bar_width / 2, accuracies, width=bar_width, label=\"Accuracy\", color=\"royalblue\")\n",
    "plt.bar(x + bar_width / 2, f1_scores, width=bar_width, label=\"F1-Score\", color=\"darkorange\")\n",
    "\n",
    "plt.xticks(x, model_names, rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Comparison: Accuracy & F1-Score\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, len(model_predictions), figsize=(len(model_predictions) * 5, 5))\n",
    "\n",
    "if len(model_predictions) == 1:  # Handle single model case\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_name, y_pred) in zip(axes, model_predictions.items()):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes, ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix: {model_name}\")\n",
    "    ax.set_xlabel(\"Predicted Labels\")\n",
    "    ax.set_ylabel(\"True Labels\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
